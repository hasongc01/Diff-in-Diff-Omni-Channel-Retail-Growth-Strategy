{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7075d3a-24df-4492-b3a5-99bc3101f885",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fe0c017-4208-4ebf-a668-c4590fab0a28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c317b225-4f5b-44f0-9294-eb4e64ca3414",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path = \"/Volumes/workspace/default/bops/BOPS data.xlsx\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8cd1c69-d53a-4ea9-a1ef-35ba30e54dc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# See sheets name\n",
    "sheets = (spark.read.format(\"excel\")\n",
    "          .option(\"operation\", \"listSheets\")\n",
    "          .load(path))\n",
    "\n",
    "display(sheets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85baf7ff-4af5-4f62-bdd3-00026e692548",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1771102925961}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read B&M\n",
    "bm = (spark.read.format(\"excel\")\n",
    "      .option(\"headerRows\", 1)                 # first row = column names\n",
    "      .option(\"dataAddress\", \"B&M Sales\")    # whole sheet\n",
    "      .option(\"inferSchema\", True)             # infer column types\n",
    "      .load(path))\n",
    "\n",
    "display(bm.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d219b418-3969-4155-ac8e-aaeff06a6340",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "online = (spark.read.format(\"excel\")\n",
    "          .option(\"headerRows\", 1)\n",
    "          .option(\"dataAddress\", \"Online Sales\")\n",
    "          .option(\"inferSchema\", True)             # infer column types\n",
    "          .load(path))\n",
    "display(online.limit(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30875a40-d069-4bb4-bb94-13f675d72b8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "bm_clean = (bm\n",
    "  .withColumnRenamed(\"id (store)\", \"id_store\")\n",
    "  .withColumn(\"date\", F.to_date(\"date\"))        # keep date-only\n",
    "  .withColumn(\"sales\", F.col(\"sales\").cast(\"double\"))  # easier for stats/plots\n",
    "  .withColumn(\"after\", F.col(\"after\").cast(\"int\")) # make binary flags explicitly int (or boolean) for clarity:\n",
    "  .withColumn(\"usa\", F.col(\"usa\").cast(\"int\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54be9c0d-c083-4060-b64d-5b1d11efa270",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# see duplicates\n",
    "print(\"rows:\", bm_clean.count())\n",
    "print(\"unique (id_store, date):\", bm_clean.select(\"id_store\",\"date\").distinct().count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90d0502f-cf4c-450f-ba18-a7e5e000ba5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "online_clean = (online\n",
    "  .withColumnRenamed(\"id (DMA)\", \"id_dma\")\n",
    "  .withColumn(\"date\", F.to_date(\"date\"))        # keep date-only\n",
    "  .withColumn(\"sales\", F.col(\"sales\").cast(\"double\"))  # easier for stats/plots\n",
    "  .withColumn(\"after\", F.col(\"after\").cast(\"int\"))\n",
    "  .withColumn(\"close\", F.col(\"close\").cast(\"int\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "310697df-726f-4f58-b940-1f3fb62a8dd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# see duplicates\n",
    "print(\"rows:\", online_clean.count())\n",
    "print(\"unique (id_dma, date):\", online_clean.select(\"id_dma\",\"date\").distinct().count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cd5e6a1-5465-4233-9c7c-6b549b2fcc70",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "save data"
    }
   },
   "outputs": [],
   "source": [
    "# save data\n",
    "bm_clean.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"bm_sales_clean\")\n",
    "online_clean.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"online_sales_clean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43b4f454-5b73-4053-b3cc-5f4bb7ef59e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2. QA Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47bb34f1-4d8e-4586-abe8-f8ff95f90b05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---------- Load dataframes if not already in memory ----------\n",
    "try:\n",
    "    bm_clean\n",
    "except NameError:\n",
    "    bm_clean = spark.table(\"bm_sales_clean\")\n",
    "\n",
    "try:\n",
    "    online_clean\n",
    "except NameError:\n",
    "    online_clean = spark.table(\"online_sales_clean\")\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def _first_existing_col(df, candidates):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def _dtype_name(df, col):\n",
    "    return dict(df.dtypes).get(col)\n",
    "\n",
    "def _is_numeric_dtype(dtype_str):\n",
    "    if dtype_str is None:\n",
    "        return False\n",
    "    d = dtype_str.lower()\n",
    "    return any(x in d for x in [\"int\", \"bigint\", \"long\", \"double\", \"float\", \"decimal\", \"smallint\", \"tinyint\"])\n",
    "\n",
    "def _is_date_like(dtype_str):\n",
    "    if dtype_str is None:\n",
    "        return False\n",
    "    d = dtype_str.lower()\n",
    "    return (\"date\" in d) or (\"timestamp\" in d)\n",
    "\n",
    "def _fail(msg):\n",
    "    raise Exception(f\"[QA FAIL] {msg}\")\n",
    "\n",
    "def _warn(msg):\n",
    "    print(f\"[QA WARN] {msg}\")\n",
    "\n",
    "def _pass(msg):\n",
    "    print(f\"[QA PASS] {msg}\")\n",
    "\n",
    "# ---------- Resolve expected columns (robust to naming) ----------\n",
    "bm_id_col   = _first_existing_col(bm_clean, [\"id_store\", \"store_id\", \"id (store)\", \"id_store \"])\n",
    "bm_date_col = _first_existing_col(bm_clean, [\"date\"])\n",
    "bm_sales_col= _first_existing_col(bm_clean, [\"sales\"])\n",
    "bm_after_col= _first_existing_col(bm_clean, [\"after\"])\n",
    "bm_usa_col  = _first_existing_col(bm_clean, [\"usa\"])\n",
    "\n",
    "on_id_col   = _first_existing_col(online_clean, [\"dma_id\", \"id_dma\", \"id (DMA)\", \"id_dma \"])\n",
    "on_date_col = _first_existing_col(online_clean, [\"date\"])\n",
    "on_sales_col= _first_existing_col(online_clean, [\"sales\"])\n",
    "on_after_col= _first_existing_col(online_clean, [\"after\"])\n",
    "on_close_col= _first_existing_col(online_clean, [\"close\"])\n",
    "\n",
    "# Hard requirements\n",
    "required_bm = [bm_id_col, bm_date_col, bm_sales_col, bm_after_col, bm_usa_col]\n",
    "required_on = [on_id_col, on_date_col, on_sales_col, on_after_col, on_close_col]\n",
    "\n",
    "if any(c is None for c in required_bm):\n",
    "    _fail(f\"B&M table missing required columns. Found columns: {bm_clean.columns}\")\n",
    "if any(c is None for c in required_on):\n",
    "    _fail(f\"Online table missing required columns. Found columns: {online_clean.columns}\")\n",
    "\n",
    "# ---------- Gate 1: Schema & type sanity ----------\n",
    "def gate_schema_types(df, id_col, date_col, sales_col, flag_cols, label):\n",
    "    # ID numeric\n",
    "    if not _is_numeric_dtype(_dtype_name(df, id_col)):\n",
    "        _fail(f\"{label}: {id_col} should be numeric; got {_dtype_name(df, id_col)}\")\n",
    "\n",
    "    # Date date/timestamp\n",
    "    if not _is_date_like(_dtype_name(df, date_col)):\n",
    "        _fail(f\"{label}: {date_col} should be date/timestamp; got {_dtype_name(df, date_col)}\")\n",
    "\n",
    "    # Sales numeric\n",
    "    if not _is_numeric_dtype(_dtype_name(df, sales_col)):\n",
    "        _fail(f\"{label}: {sales_col} should be numeric; got {_dtype_name(df, sales_col)}\")\n",
    "\n",
    "    # Flags numeric (int/bigint/etc.)\n",
    "    for fcol in flag_cols:\n",
    "        if not _is_numeric_dtype(_dtype_name(df, fcol)):\n",
    "            _fail(f\"{label}: {fcol} should be numeric flag (0/1); got {_dtype_name(df, fcol)}\")\n",
    "\n",
    "    _pass(f\"{label}: schema/type sanity checks passed.\")\n",
    "\n",
    "gate_schema_types(\n",
    "    bm_clean, bm_id_col, bm_date_col, bm_sales_col, [bm_after_col, bm_usa_col], \"B&M\"\n",
    ")\n",
    "gate_schema_types(\n",
    "    online_clean, on_id_col, on_date_col, on_sales_col, [on_after_col, on_close_col], \"Online\"\n",
    ")\n",
    "\n",
    "# ---------- Gate 2: Primary key uniqueness ----------\n",
    "def gate_key_uniqueness(df, key_cols, label, sample=20):\n",
    "    dup = (df.groupBy(*key_cols).count().filter(F.col(\"count\") > 1))\n",
    "    ndup = dup.count()\n",
    "    if ndup > 0:\n",
    "        _warn(f\"{label}: found {ndup} duplicate keys on {key_cols}. Showing examples:\")\n",
    "        display(dup.orderBy(F.desc(\"count\")).limit(sample))\n",
    "        _fail(f\"{label}: key uniqueness failed for {key_cols}.\")\n",
    "    _pass(f\"{label}: key uniqueness passed for {key_cols}.\")\n",
    "\n",
    "gate_key_uniqueness(bm_clean, [bm_id_col, bm_date_col], \"B&M\")\n",
    "gate_key_uniqueness(online_clean, [on_id_col, on_date_col], \"Online\")\n",
    "\n",
    "# ---------- Gate 3: Flag domain validity (must be 0/1) ----------\n",
    "def gate_binary_flags(df, flag_cols, label):\n",
    "    for c in flag_cols:\n",
    "        bad = df.select(c).where((F.col(c).isNotNull()) & (~F.col(c).isin([0, 1]))).count()\n",
    "        if bad > 0:\n",
    "            _warn(f\"{label}: flag {c} has {bad} rows not in {{0,1}}. Distinct values:\")\n",
    "            display(df.select(c).distinct().orderBy(c))\n",
    "            _fail(f\"{label}: binary flag check failed for {c}.\")\n",
    "    _pass(f\"{label}: binary flag checks passed for {flag_cols}.\")\n",
    "\n",
    "gate_binary_flags(bm_clean, [bm_after_col, bm_usa_col], \"B&M\")\n",
    "gate_binary_flags(online_clean, [on_after_col, on_close_col], \"Online\")\n",
    "\n",
    "# ---------- Gate 4: Completeness (no nulls in key + critical fields) ----------\n",
    "def gate_not_null(df, cols, label):\n",
    "    null_counts = {c: df.where(F.col(c).isNull()).count() for c in cols}\n",
    "    bad = {c: n for c, n in null_counts.items() if n > 0}\n",
    "    if bad:\n",
    "        _warn(f\"{label}: nulls found in critical columns: {bad}\")\n",
    "        _fail(f\"{label}: not-null gate failed.\")\n",
    "    _pass(f\"{label}: not-null gate passed for {cols}.\")\n",
    "\n",
    "gate_not_null(bm_clean, [bm_id_col, bm_date_col, bm_sales_col, bm_after_col, bm_usa_col], \"B&M\")\n",
    "gate_not_null(online_clean, [on_id_col, on_date_col, on_sales_col, on_after_col, on_close_col], \"Online\")\n",
    "\n",
    "# ---------- Gate 5: Value sanity (sales non-negative) + Outlier WARN ----------\n",
    "def gate_sales_sanity(df, id_col, date_col, sales_col, label):\n",
    "    neg = df.where(F.col(sales_col) < 0).count()\n",
    "    if neg > 0:\n",
    "        _warn(f\"{label}: found {neg} rows with negative {sales_col}. Showing examples:\")\n",
    "        display(df.where(F.col(sales_col) < 0).select(id_col, date_col, sales_col).limit(20))\n",
    "        _fail(f\"{label}: sales sanity failed (negative sales).\")\n",
    "    _pass(f\"{label}: sales non-negative check passed.\")\n",
    "\n",
    "    # Soft outlier diagnostics (WARN only)\n",
    "    qs = df.approxQuantile(sales_col, [0.5, 0.99, 0.999], 0.001)\n",
    "    if len(qs) == 3:\n",
    "        median, p99, p999 = qs\n",
    "        print(f\"{label}: {sales_col} quantiles -> median={median:.4f}, p99={p99:.4f}, p999={p999:.4f}\")\n",
    "        # Show top 10 for inspection\n",
    "        display(df.select(id_col, date_col, sales_col).orderBy(F.desc(sales_col)).limit(10))\n",
    "        if median > 0 and (p999 / median) > 1_000:\n",
    "            _warn(f\"{label}: extreme tail detected (p999/median > 1000). Consider log1p(sales) robustness.\")\n",
    "    else:\n",
    "        _warn(f\"{label}: could not compute quantiles for {sales_col} (unexpected).\")\n",
    "\n",
    "gate_sales_sanity(bm_clean, bm_id_col, bm_date_col, bm_sales_col, \"B&M\")\n",
    "gate_sales_sanity(online_clean, on_id_col, on_date_col, on_sales_col, \"Online\")\n",
    "\n",
    "# ---------- Gate 6: Time coverage sanity (weekly completeness pattern) ----------\n",
    "def gate_time_coverage(df, date_col, label):\n",
    "    # counts per week-date\n",
    "    counts = df.groupBy(date_col).count()\n",
    "    n_dates = counts.count()\n",
    "    if n_dates == 0:\n",
    "        _fail(f\"{label}: no dates found in {date_col}.\")\n",
    "    _pass(f\"{label}: found {n_dates} unique {date_col} values.\")\n",
    "\n",
    "    # Flag weeks with unusually low coverage (WARN only)\n",
    "    median_cnt = counts.approxQuantile(\"count\", [0.5], 0.001)[0]\n",
    "    low = counts.where(F.col(\"count\") < F.lit(0.5) * F.lit(median_cnt)).orderBy(\"count\")\n",
    "    low_n = low.count()\n",
    "    if low_n > 0:\n",
    "        _warn(f\"{label}: {low_n} weeks have <50% of median row coverage (median={median_cnt}). Review missingness.\")\n",
    "        display(low.limit(25))\n",
    "    else:\n",
    "        _pass(f\"{label}: weekly coverage looks stable (no weeks <50% median coverage).\")\n",
    "\n",
    "    # min/max date\n",
    "    mm = df.select(F.min(date_col).alias(\"min_date\"), F.max(date_col).alias(\"max_date\")).collect()[0]\n",
    "    print(f\"{label}: date range = {mm['min_date']} to {mm['max_date']}\")\n",
    "\n",
    "gate_time_coverage(bm_clean, bm_date_col, \"B&M\")\n",
    "gate_time_coverage(online_clean, on_date_col, \"Online\")\n",
    "\n",
    "print(\"\\nâœ… All HARD QA gates passed. Soft warnings (if any) are listed above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c60220e5-4970-40db-a97a-6f318fdeb4dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 3. Single Source of Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a49988ea-803e-4cdb-ac8e-cade48e8834e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "on = spark.table(\"online_sales_clean\")\n",
    "\n",
    "did_panel = (\n",
    "    on\n",
    "    # keep only what you need (optional: keep year/week too)\n",
    "    .select(\n",
    "        F.col(\"id_dma\").cast(\"bigint\").alias(\"id_dma\"),\n",
    "        F.to_date(\"date\").alias(\"date\"),\n",
    "        F.col(\"year\").cast(\"int\").alias(\"year\"),\n",
    "        F.col(\"week\").cast(\"int\").alias(\"week\"),\n",
    "        F.col(\"after\").cast(\"int\").alias(\"after\"),\n",
    "        F.col(\"close\").cast(\"int\").alias(\"close\"),\n",
    "        F.col(\"sales\").cast(\"double\").alias(\"sales\"),\n",
    "    )\n",
    "    # create interaction term\n",
    "    .withColumn(\"treated_post\", (F.col(\"close\") * F.col(\"after\")).cast(\"int\"))\n",
    "    # enforce one row per DMA-week\n",
    "    .dropDuplicates([\"id_dma\", \"date\"])\n",
    ")\n",
    "\n",
    "did_panel.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"did_panel\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6635078673361625,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "DiD Retail Analytics",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
